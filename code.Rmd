---
title: "Mass Spec Aging Data Analysis"
author: "Samuel Leung"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load and Preprocess data}
data <- read.csv("data/cardio_data_processed.csv")
head(data)
```

```{r Finding size of dataset}
nrow(data)
```

## Goal

-   To estimate the total number of people with cardiovascular disease: $\hat{t}_{\text{cardio}}$
-   To estimate the average blood pressure of the population: $\bar{u}_{\text{ap_hi}}, \hspace{1mm}\bar{u}_{\text{ap_lo}}$

## Sample Size Calculations:

We can't find the maximum variance from a continuous variable, we go through sample size calculations with the maximum variance possible from a binary variable: the total number of people with cardiovascular disease. We want the margin of error $\delta$ of our estimate to be within $\pm$ 1000 people, 19 times out of 20.

### Mathematical steps

Let $X$ be the binary random variable that models the number of people with cardiovascular disease, with mean $p$ and variance $p(1-p)$. Let $\bar y_S = \hat p$ be the estimate of the average number people with cardiovascular disease. of We want to estimate the total number of people with cardiovascular disease, $\hat t_{p,\text{cardio}} = N \bar y_s$. The variance of this estimator will be $var(\hat t_{p, \text{cardio}}) = N^2\hat p(1-\hat p)$. Going through our sample size calculations, we want to maximize variance, so we take $\hat p = 0.5$. Therefore, ignoring the finite population correction factor for now, we want to solve the following:

$$
\begin{align*}
1000 &\geq 1.96\sqrt{0.25 N^2 / n_0}\\
\implies \Big(\frac{1000}{1.96}\Big)^2 &\geq \frac{N^2}{4n_0}\\
\implies n_0 &\geq \Big(\frac{N}{2} \cdot \frac{1.96}{1000}\Big)^2\\
\end{align*}
$$

Converting this to code:

```{r Sample Size Calc}
# add reasoning for why we chose MOE to be 1000

N <- nrow(data)
target <- 1000
n_0 <- (1.96 * N / (2*target))^2
print(paste0("sample size ignoring FPC: ", n_0))
print(paste0("n_0 / N: ", n_0 / N))
```

Comparing the sample size calculated above with the total population size, we see that $\frac{n_0}{N} = 0.066 \geq 0.05$. So we cannot ignore the finite population correction factor in our sample size calculations. Factoring in the FPC, we would want to solve the following: $$n = \frac{n_0}{1+n_0/N}$$ Doing the calculations again with the FPC in mind with code:

```{r Sample Size with FPC}
n <- ceiling(n_0 / (1 + n_0 / N))
print(paste0("Sample Size with FPC: ", n))
```

So we use a sample size of 4194.

```{r Generate SRS}
# set seed for reproducable results
set.seed(124)

# generate random sample
srs <- data[sample(1:nrow(data), n), ]

# double check size of sample
dim(srs)
```

```{r Calculate vanilla estimates with SRS}
fpc <- 1 - n / N

ap_hi.est <- round(mean(srs$ap_hi), 3)
ap_hi.var <- fpc*var(srs$ap_hi) / n
ap_hi.moe <- round(1.96*sqrt(ap_hi.var), 3)

ap_lo.est <- round(mean(srs$ap_lo), 3)
ap_lo.var <- fpc*var(srs$ap_lo) / n
ap_lo.moe <- round(1.96*sqrt(ap_lo.var), 3)

cardio.p <- mean(srs$cardio)
cardio.est <- round(N*cardio.p, 3)
cardio.var <- fpc*(cardio.p)*(1-cardio.p) * N
cardio.moe <- round(1.96*sqrt(cardio.var), 3)

print(paste0("Average ap_hi estimate: ", ap_hi.est, " +/- ", ap_hi.moe))
print(paste0("Average ap_lo estimate: ", ap_lo.est, " +/- ", ap_lo.moe))
print(paste0("Total cardio estimate: ", cardio.est, " +/- ", cardio.moe))
```
## Regression and Ratio Estimates

To calculate regression and ratio estimates, we want to use an auxiliary variable with strong correlation. We do so by first calculating the correlation values for each of our target variables against the other numeric columns in our dataset:

```{r Regression + Ratio estimates from Andrew}
# ngl looks incorrect so gonna recode the estimates + SE myself
pop.cor <- cor(data[,2:15])
target_lst <- c("ap_hi", "ap_lo", "cardio")
pop.cor[target_lst, !(colnames(pop.cor)%in%target_lst)]
```
The highest positive-correlating variable for `ap_hi` and `ap_lo` are `weight`. The highest positively-correlating variable for `cardio` is `age`, however it is difficult to interpret the value in this column, as the age is not counted by year. So we opt to use the `age_years` column, which has similar correlation values to `age`. We note that for all these correlations, none of them seem particularly strong, which suggests that the ratio and regression estimates may not be that useful in prediction. 
```{r}
# ratio estimate:
ratio.ap_hi.est <- mean(srs$ap_hi) / mean(srs$weight)
ratio.ap_lo.est <- mean(srs$ap_lo) / mean(srs$weight)
ratio.cardio.est <- mean(srs$cardio) / mean(srs$age_years)

ratio.ap_hi.se <- sqrt(fpc*sum(var(srs$ap_hi-ratio.ap_hi.est*srs$weight))/n)
ratio.ap_lo.se <- sqrt(fpc*sum(var(srs$ap_lo-ratio.ap_lo.est*srs$weight))/n)
ratio.cardio.se <- sqrt(fpc*sum(var(srs$cardio-ratio.cardio.est*srs$age_years))/n)

ratio.summary <- rbind(
  c(ratio.ap_hi.est, ratio.ap_hi.se),
  c(ratio.ap_lo.est, ratio.ap_lo.se),
  c(ratio.cardio.est, ratio.cardio.se)
)
colnames(ratio.summary) <- c("est", "se")
rownames(ratio.summary) <- paste("ratio.", target_lst, sep="")
ratio.summary
```


```{r Regression Estimates}
# regression estimate:

# helper function for regression estimate. X, y should be matrices of proper size. Don't ask me why i'm not just using lm() for this; idk either
regression.est_and_se <- function(X, y, x.pop) {
  X <- as.matrix(X); y <- as.matrix(y)
  X <- cbind(rep(1, dim(X)[1]), X)
  w <- solve(t(X)%*%X)%*%t(X)%*%y
  y.hat <- X%*%w
  se <- sqrt(fpc*sum(var(y - y.hat))/nrow(X))
  est <- cbind(1, x.pop) %*% w
  
  ret <- c()
  ret$est <- est; ret$se <- se
  return(ret)
}

reg.ap_hi <- regression.est_and_se(srs$weight, srs$ap_hi, mean(data$weight))
reg.ap_lo <- regression.est_and_se(srs$weight, srs$ap_lo, mean(data$weight))
reg.cardio <- regression.est_and_se(srs$age_years, srs$cardio, mean(data$age_years))

reg.summary <- rbind(
  reg.ap_hi, reg.ap_lo, reg.cardio
)

reg.summary
```


## Stratification

To determine which variable to stratify by, we base our decision on the variable that would produce the lowest within-strata variance. Equivalently, from a chosen subset of potential stratifying candidates $s_i \in \{s_1 \cdots s_n\}$, we want to choose to stratify on $s_i$ if: $$ \text{argmax}\big(s^2_{\text{between strata}} / s^2_{\text{total}}\big) = s_i$$ $s^2_{\text{between strata}}$ and $s^2_{\text{total}}$ are between-strata and total variances respectively. Calculating this ratio for the potential stratifying variables `gender`, `cholesterol`, `gluc`, `smoke`, `alco`, and `active`:

```{r Stratified sampling EDA}
# should stratify based on variable that produces smallest within-strata variance

# candidate stratifiying variables: 
# gender
# cholesterol
# gluc
# smoke
# alco
# active

# Calculate within-strata variance of target based off of levels of variables in var_lst
strata.compare_var <- function(df, target_df) {
  var_lst <- colnames(df)
  target_lst <- colnames(target_df)
  ret <- NULL
  
  for (t in 1:length(target_df)) {
    target <- target_df[,t]
    ratios <- rep(NA, length(var_lst))
    for (i in 1:length(var_lst)) {
      v <- df[,i]
      var.ws <- sum(prop.table(table(v)) * tapply(target, v, var))
      var.bs <- sum(prop.table(table(v)) * (tapply(target, v, mean) - mean(target))^2)
      var.tot <- var.ws + var.bs
      ratios[i] <-  var.ws / var.tot
    }
    ret <- rbind(ret, ratios)
  }
  rownames(ret) <- target_lst
  colnames(ret) <- var_lst
  return(ret)
}

pred <- data[,c("gender", "cholesterol", "gluc", 
                "smoke", "alco", "active")]
target <- data[,c("cardio", "ap_hi", "ap_lo")]
strata.compare_var(pred, target)
```

From the table above, we see that variables `smoke`, `alco`, and `active` have the highest between-strata to overall variance ratios, indicating that these variables are the best to stratify on. These three variables all have similar performance, so we arbitrarily choose one (go with `alco` for now).

We now want to see how we much from each strata we should sample. In our case, cost of sampling in our case is constant, as our sampling process is simulated. We want to see if we can use proportional allocation, so we check to see if we can assume the variances of each strata are equal:

```{r Not sure how to verify this}

```

*REASONING TO ASSUME THAT VARIANCE IS NOT EQUAL*, so optimal allocation in our case can be calculated through the formula below:

$$
\begin{align*}
 n_h &= n\Big(\frac{N_hs_{h, \text{guess}}}{\sum_{i=1}^hN_is_{i, \text{guess}}}\Big)
\end{align*}
$$

According to the documentation from Kaggle, the `alco` predictor has 2 levels: $0$ for people that do not consume alcohol, and $1$ for people that do. We guess the standard deviation of each strata using stratum-specific population variance. Taking total sample size `n` to be the same as above, we find the proportional size for each strata:

```{r Optimum Allocation Calculations}
strata.ap_hi.weight <- table(data$alco)*tapply(data$ap_hi, data$alco, sd)
strata.ap_lo.weight <- table(data$alco)*tapply(data$ap_lo, data$alco, sd)
strata.cardio.weight <- table(data$alco)*tapply(data$cardio, data$alco, sd)

rbind(
  strata.ap_hi.weight / sum(strata.ap_hi.weight),
  strata.ap_lo.weight / sum(strata.ap_lo.weight),
  strata.cardio.weight / sum(strata.cardio.weight)
)
```

\*\*DETERMINE PROPORTIONAL SAMPLE SIZE HERE\*\*

```{r General code for finding stratified estimate}
set.seed(124)

# generate stratified sample
prop <- 0.94
n1 <- round(prop * n, 0); n2 <- round((1-prop) * n, 0)
pop.alco_zero <- data[data$alco==0,]; pop.alco_one <- data[data$alco==1,]
strata.sample <- rbind(pop.alco_zero[sample(1:nrow(pop.alco_zero), n1),],
                       pop.alco_one[sample(1:nrow(pop.alco_one), n2), ])


# given population data, sample and calculate the stratified estimate + SE
stratify.est_and_se <- function(pop, sample, var_str, target_str) {
  pop.var.count <- table(data[,var_str])
  sample.var.count <- table(sample[,var_str])
  # population proportion for each strata
  pop.prop <- prop.table(pop.var.count)
  
  sample.means <- tapply(sample[,target_str], sample[,var_str], mean)
  sample.vars <- tapply(sample[,target_str], sample[,var_str], var) / pop.var.count
  fpc <- 1 - sample.var.count / pop.var.count
  
  est <- round(sum(pop.prop * sample.means), 3)
  se <- round(sqrt(sum(pop.prop^2 * fpc * sample.vars)), 3)
  print(paste0("Strata Estimate for ", target_str, ": ", est, " +/- ", 1.96*se))
  
  ret <- c()
  ret$est <- est
  ret$se <- se
  return(ret)
}

stratify.est_and_se(data, strata.sample, var_str = "alco", target_str = "ap_hi")
stratify.est_and_se(data, strata.sample, var_str = "alco", target_str = "ap_lo")
stratify.est_and_se(data, strata.sample, var_str = "alco", target_str = "cardio")
```

```{r TODO}
# Add/ modify Ratio + Regression Estimates from Andrew
# Finish up code for stratified estimator
```
